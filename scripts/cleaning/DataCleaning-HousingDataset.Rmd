---
title: "Ireland Housing - Data Cleaning"
author: Marcos Cavalcante
date: '`r Sys.Date()`'
output: 
  github_document:
    toc: true
    toc_depth: 3
    number_sections: false
always_allow_html: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
options(dplyr.summarise.inform = FALSE)
```


### Data Cleaning

In this part, the dataset will be cleant so it can be used more appropriately for data exploration and use by the machine learning techniques.

The following steps will be taken:

1. Loading the dataset
2. Removal of duplicates
3. Handling of missing values
4. Removal of unnecessary variables
5. Renaming variables
6. Conversion into appropriate data types
7. Creation of new variables
8. Handling outliers
9. Scaling variables
10. Write Clean Dataset

## Installing libraries

In this pre-step, all of the required packages will be installed and loaded.

```{r, Package Installation, results=FALSE, message=FALSE, warning=FALSE}
packages <- c("tidyverse", "haven", "devtools", "dplyr", "stringr", "kableExtra", 
              "formattable","stringi", "see", "ggraph", "correlation", 
              "PerformanceAnalytics", "gridExtra")

if(sum(as.numeric(!packages %in% installed.packages())) != 0){
  installer <- packages[!packages %in% installed.packages()]
  for(i in 1:length(installer)) {
    install.packages(installer, dependencies = T)
    break()
  }
  sapply(packages, require, character = T) 
} else {
  sapply(packages, require, character = T) 
}

devtools::install_github("ropensci/skimr")
library(skimr)
```


## Loading the Dataset - Ireland Housing dataset

At this step, the Ireland houses dataset will be imported.

```{r Data Import}
dataset_directory <- "../../datasets/"
dataset_filename <- paste(dataset_directory, "house_listings_all.csv", sep="")

options(scipen = 999) # turn off scientific notation

ireland_houses <- read.csv(file = dataset_filename) # Load the dataset
```

#### Loading the Dataset - First look at the dataset

In this step, the first five rows of our dataset will be displayed so that we can take a look at the different pieces of data available to us and what kind of information they bring to the analysis.

```{r Visualizing first 5 rows}
summary(ireland_houses, 5)
```

Looking at the output of the previous step, some interesting things can be observed:

* __id__ and __daftShortCode__ seem to describe values that look like identifiers, but identifiers that are used for different purposes.

* __title__ contains values which look like addresses of properties.

* __price__ are expected to be of _numeric_ type, nevertheless, strings like _"Price on Application"_ are also present.

* __size_meters_squared__ and __propertySize__, at first glance, contain the same information / values, however, formatted differently. Another interesting point is that there are values missing on both variables.

* __bedrooms__ and __bathrooms__ contain the amount of each of those in the property and should therefore be of _numeric_ type.

* __propertyType__ contain values that describe what kind of property that is, for example: Bungalow and Detached.

* __publishDate__ is an attribute related more to the ad than to the property itself and it describes the date when the ad was published.

* __ber_rating__ is a variable that tells how energy efficient is a property, BER stands for Building Energy Rating.

* __ber_code__ is simply the ID of the certificate that the house was given.

* __ber_epi__ describes the energy consumption per square meter of a the property yearly.

* __latitude__ and __longitude__ are spatial values used to locate the property on the map.

* __category__ only shows the value of _"Buy"_.

* __location__ contains value with the concatenation of town/city and the county.

* __url_link__ contains the value of the URL of the house on the platform's website.


## Removal of duplicates

In this step the goal is to remove any observation that is duplicated in the dataset. This is going to be done by searching for observations that contain the same _id_ and _url_link_ values.

It is worth noting that currently the dataset has over 100 thousand observations, after removing the duplicate observations, just under 13 thousand observations will remain in the dataset.

```{r Removing duplicates}

print(paste("Number of observations BEFORE removing duplicates:", 
            nrow(ireland_houses)))

remove_duplicates <- function( dfHouses, dupes_column ) {
  subsetDfHouse <- dfHouses[dupes_column]
  return ( dfHouses[!duplicated(subsetDfHouse),] )
}

ireland_houses <- remove_duplicates(ireland_houses, c("id", "url_link"))

print(paste("Number of observations AFTER removing duplicates:", 
            nrow(ireland_houses)))

```



## Handling missing values - What is the % of values missing for each variable.

In this step, it is possible to see that there are variables with missing values. Nevertheless, it is worth noting that most of those features have been assigned the type of character and there might be features with missing data, but due to the wrong data type, they are displayed as if they were not missing.

At a later point, those features with the wrong data type will be converted into the appropriate data type.


```{r Handling Missing Values}

compile_missing_values_table <- function( dfHouses ) {
  percentage_missing <- colSums(is.na(dfHouses)) * 100 / nrow(dfHouses)
  percentage_missing_sorted <- percentage_missing[
    order(percentage_missing, decreasing = TRUE)
  ]

  percentage_missing_formatted = percent(
    formattable(percentage_missing_sorted) / 100
  )

  kbl(percentage_missing_formatted, col.names = NULL) %>%
    kable_paper(bootstrap_options = "striped", 
                full_width = F, 
                html_font = "Computer Modern") %>%
    add_header_above(header = c("Feature", "Percentage of Missing Values"))
}


compile_missing_values_table( ireland_houses )

```


As we can see from the output of the step above, _ber_code_ is missing in 42.47% of the observations (5461 rows), _size_meters_squared_ is missing in 26.54% of the observations (3413 rows) and finally, _bathrooms_ is missing in 2.05% of the observations (263 rows).

### Handling missing values - BER Code

As the _ber_code_ variable only represents the certificate number of the BER rating (Building Energy Rating), we can safely remove this column from the dataset as the certificate number will be individually assigned to each property and do not actually influence on the house price, instead the house's BER rating and BER EPI (Energy Performance Indicator) might, in fact, affect the house price.

```{r - Remove BER Code variable}

ireland_houses <- ireland_houses[ , !names(ireland_houses) %in% c("ber_code") ]

```


### Handling missing values - Size Meters Squared and Bathrooms

The observations with no values for _size_meters_squared_ and _bathrooms_  will also be removed to keep the dataset as clean as possible and, no technique for inputing those missing values will be used not to introduce any bias in the study.

```{r - Remove observations with missing data for bathrooms and size_meters_sqaured }

ireland_houses <- ireland_houses %>% filter(!is.na(bathrooms))
ireland_houses <- ireland_houses %>% filter(!is.na(size_meters_squared))

```


Finally, It can be seen that there does not seem to be any values missing. It will be investigated further later on whether or not this is true when the data type conversion is performed. 

There are over 9 thousand observations remaining in our dataset.

```{r Handling Missing Values - Printing values}

compile_missing_values_table( ireland_houses )

```



## Removal of unnecessary variables

In order to keep the dataset concise and not be using too much unnecessary data, let's remove some other variables: _url_link_, _id_ and _daft_short_code_ as those represent some sort of identifier. 

Other variables that can be removed are _publishDate_ as it only tells us when the ad was put up on the website, _category_ as nearly all but 7 observations have the same value of "Buy", the others have a value of "New Homes" and finally _propertySize_ as that information is already present in a variable called "size_meters_squared".

```{r - Remove urlLink, id, daftShortCode, propertySize and publishDate}

vars_to_remove <- c( "url_link", "id", "daftShortcode", "publishDate", "category", "propertySize")

ireland_houses <- ireland_houses[ , !names(ireland_houses) %in% vars_to_remove ]

```



## Describing the dataset with _skim_ function

Let's see again what the dataset looks like and how the values across the variables in it are statistically distributed with the function _skim_.

```{r - describing the dataset with skim}
skim(ireland_houses)
```

## Renaming Variables

It can be seen that some of the variables do not follow a naming standard or use an adequate name, for example: *title* actually refers to the *address* of the house and *size_meters_squared* refers to the *size*.

The naming convention used across the variables is going to be __camelCase__, therefore *ber_rating* and *ber_epi* will be renamed to *berRating* and *berEPI* respectively.

```{r Data Transformation - Renaming Variables}
ireland_houses <- rename( ireland_houses, 
  address = title,
  size = size_meters_squared,
  berRating = ber_rating,
  berEPI = ber_epi,
)
```


## Conversion into appropriate data types

The output of the __skim__ function has showed some interesting details, for example: *price*, *berEPI*, and *bedrooms* are of type __character__, when __numeric__ would be, in fact, a better type because they represent quantitative data.

*propertyType* and *berRating* are of type characters, however, __factor__ may be a better data type because there is a limited number of categorical values those can hold, __factor__ is also a better data type as it allows for better data manipulation so that typos can be avoided and sorting the data in a meaningful way becomes possible.

Thus, in this next step, the values in those variables will be converted into more suitable data types.

```{r Data Transformation - Tidying up the data, warning=FALSE}

strs_to_replace_with_na <- c( "", "Price on Application",
                              "AMV: Price on Application")

ireland_houses <- mutate(ireland_houses,
  
  berEPI = as.numeric(gsub("kWh/m2/yr", "", berEPI)),
  
  bedrooms = as.numeric(bedrooms),
  
  propertyType = as.factor(propertyType),
  
  berRating = as.factor(berRating),
  
  price = as.numeric(format(
    (parse_number(gsub(",", "", price), strs_to_replace_with_na)), 
    scientific = FALSE, 
    big.mark ="")),
)
```


### Fixing properties with empty factor for property type

It was noted that 2 properties have an empty property type. In order to fix it, the house address was used to look it up online. Suitable property types were found for those 2 observations and they are going to be applied now.

```{r - fixing property type of 2 observations}

propertyLevels <- levels(ireland_houses$propertyType)
semiDetached <- propertyLevels[7]
semiDetached <- factor( semiDetached, levels = propertyLevels )

detached <- propertyLevels[4]
detached <- factor( detached, levels = propertyLevels )

ireland_houses <- ireland_houses %>% 
  mutate( 
    propertyType = if_else( propertyType=="" & 
                            address == "Four bedroomed Semi-Detached, Owenmore Paddock, Ballinacarrow, Co. Sligo", 
                            semiDetached, 
                            propertyType ) 
  )

ireland_houses <- ireland_houses %>% 
  mutate( 
    propertyType = if_else( propertyType=="" & 
                            address == "House Tpye D, Moin Na Ri, Moin Na Ri, Kilworth, Co. Cork", 
                            detached, 
                            propertyType ) 
  )

ireland_houses$propertyType <- factor(ireland_houses$propertyType)

```

Let's describe the dataset one more time to see what it looks like now.

```{r Describing the dataset with skim again, warning=FALSE}

skim(ireland_houses)

```


## Handling of missing values - Removing observations and variables

Once the date types have been correctly typed, it is possible to see the presence of observations with missing number of __bedrooms__, only 2, and also that about 44% of the observations are missing a value for __berEPI__. Finally, there are also 298 observations without __price__, which is the target variable of this study, so those can also be removed.

For the 2 observations that do not have a valid value for __bedrooms__ and 298 observations without __price__, those will be removed. The variable __berEPI__ will also be removed as it is missing in nearly half of the observations.

```{r - remove berEPI and observations with missing bedrooms and price,  warning=FALSE}

ireland_houses <- ireland_houses[ , !names(ireland_houses) %in% c("berEPI") ]
ireland_houses <- ireland_houses %>% filter(!is.na(bedrooms))
ireland_houses <- ireland_houses %>% filter(!is.na(price))

```



## Creation of new variables

In this step, we will try create new variables in order to try gain more insight into the data.
The variables that will be created are derived from __address__ and __location__. 
The goal is to try create a variable which helps us identify the neighboorhood of the property as best as possible.

```{r - creating new variables}

parse_townOrNeighbourhood <- function(location) {
  
  str_tokens_vec <- str_split( location, "_" )[[1]]
  town_or_neighbourhood <- ""
  
  if (length(str_tokens_vec) == 1) {
    town_or_neighbourhood <-  str_tokens_vec[1]
  } else {
    town_or_neighbourhood <- stri_paste(str_tokens_vec[1 : length(str_tokens_vec)-1], collapse = "_"  )
  }
  
  return (town_or_neighbourhood)
}


parse_county <- function(location) {
  
  str_tokens_vec <- str_split( location, "_" )[[1]]
  county_token <- str_tokens_vec[length(str_tokens_vec)]
  
  if (tolower(county_token) == "city") {
    county_token <- str_tokens_vec[1]
  }
  
  return (county_token)
}


ireland_houses$county <- ireland_houses$location %>% 
  lapply( parse_county ) %>% 
  unlist %>% 
  as.factor


ireland_houses$townOrNeighbourhood <- ireland_houses$location %>% 
  lapply( parse_townOrNeighbourhood ) %>% 
  unlist %>% 
  as.factor

```


## Handling Outliers

This step is focused on trying to find out outliers in our dataset.
Outliers are not desirable as they can have great impact on the statistical analyses of a machine learning project.


Let's identify the outliers in our dataset. Only numerical variables will be analysed initially as those can be found more easily with techniques such as: histograms, box plots and z-score. Thus, the variables that will be analysed are: price, size, bedrooms and bathrooms.


```{r - create histograms and plot, fig.height = 13, fig.width = 18}


create_annotations <- function( dfVariable, statsPosition ) {
  
  annotations <- data.frame(
    x = c(
      round(min(dfVariable), 2), 
      round(mean(dfVariable), 2), 
      round(max(dfVariable), 2)
    ),
    y = statsPosition,
    label = c("Min:", "Mean:", "Max:")
  ) 
  
  return (annotations)
}


create_histogram <- function( dataframe, props, labs, annotations ) {

  histogram <- ggplot(dataframe$df, aes(x = !!dataframe$col)) +
      geom_histogram(color = props$color, fill = props$fill, bins = props$bins) +
      labs( title = labs$title,
            x = labs$x,
            y = labs$y
      ) +
      scale_fill_brewer() +
      geom_text(data = annotations,
                aes(x = x, y = y, label = paste(label, x)),
                size = 4,
                fontface="bold")
  
  return (histogram)
}


sizeHistogram <- create_histogram( 
  dataframe = list( df = ireland_houses, col = sym("size") ),
  props = list(color = "darkblue", fill = "lightblue", bins = 30),
  labs  = list(title = "Histogram of house sizes", x = "Size", y = "Observations"),
  annotations = create_annotations(ireland_houses$size,  c(4100, 4700, 200))
)


priceHistogram <- create_histogram( 
  dataframe = list( df = ireland_houses, col = sym("price") ),
  props = list(color = "darkgreen", fill = "lightgreen", bins = 30),
  labs  = list(title = "Histogram of house prices", x = "Price", y = "Observations"),
  annotations = create_annotations(ireland_houses$price,  c(2800, 5500, 100))
)

bathroomsHistogram <- create_histogram( 
  dataframe = list(df = ireland_houses, col = sym("bathrooms") ),
  props = list(color = "deeppink", fill = "lightpink", bins = 15),
  labs  = list(title = "Histogram of number of bathrooms per house", x = "Bathrooms", y = "Observations"),
  annotations = create_annotations(ireland_houses$bathrooms,  c(5600, 3500, 100))
)

bedroomsHistogram <- create_histogram( 
  dataframe = list(df = ireland_houses, col = sym("bedrooms") ),
  props = list(color = "firebrick", fill = "orangered", bins = 15),
  labs  = list(title = "Histogram of number of bedrooms per house", x = "Bedrooms", y = "Observations"),
  annotations = create_annotations(ireland_houses$bedrooms,   c(500, 5000, 100))
)


grid.arrange(
  arrangeGrob(sizeHistogram, priceHistogram),
  arrangeGrob(bedroomsHistogram, bathroomsHistogram), ncol = 2)



```

The histograms above show some interesting results in our dataset. For example, it can be observed that there are properties with over 20 bathrooms or 20 bedrooms, which looks quite unrealistic at first sight, but when you look it up those properties, you can clearly see that those are mansions. This helps us to also understand the disparity in the price of the house, where the minimum cost is about €40,000.00 and the mean is €462,205.00, nevertheless the maximum price is €15,000,000.00.

The house size histogram does show us some clear outliers however. Those outliers are houses with a size under 20 square meters for example. There are 10 observations that fall under that filter, in fact, all of those observations are under 16 square meters. Looking up the properties in those 10 address, it is possible to see that those properties have been incorrectly input in the dataset, so they can be filtered out.


```{r - Removing outliers where size is less than 17 square meters}
ireland_houses <- ireland_houses %>% filter(!(size < 17))
```


## Scaling variables

This step will not be performed at this point, as some data exploration activities are necessary to be carried out beforehand.

## Write Clean Dataset to disk

Finally, after doing some initial data exploration and cleaning, we can proceed to start analyzing the variance and covariance of the variables in our dataset, thus diving a little deeper into our analysis.

```{r Write Clean Dataset to Disk}


dataset_filename <- paste(dataset_directory, 
                          "ireland_houses_cleaned.Rda", 
                          sep="")

save(ireland_houses, file = dataset_filename)
```

