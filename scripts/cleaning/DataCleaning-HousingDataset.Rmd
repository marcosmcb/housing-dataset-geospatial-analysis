---
title: "Ireland Housing - Data Cleaning"
author: Marcos Cavalcante
output: github_document
always_allow_html: true
---



### Installing libraries

First step is to install and load the necessary libraries.

```{r, Package Installation, include=FALSE}
packages <- c("tidyverse","haven", "devtools", "dplyr", "stringr", "kableExtra", "formattable","stringi", "see", "ggraph", "correlation", "PerformanceAnalytics")

if(sum(as.numeric(!packages %in% installed.packages())) != 0){
  installer <- packages[!packages %in% installed.packages()]
  for(i in 1:length(installer)) {
    install.packages(installer, dependencies = T)
    break()
  }
  sapply(packages, require, character = T) 
} else {
  sapply(packages, require, character = T) 
}

devtools::install_github("ropensci/skimr")
library(skimr)

```

## Import the Ireland Housing dataset

At this step, the Ireland houses dataset will be imported

```{r Data Import}
dataset_directory <- "../../datasets/"
dataset_filename <- paste(dataset_directory, "house_listings_all.csv", sep="")

ireland_houses <- read.csv(file = dataset_filename) # Load the dataset
```

## Data Exploration

In this step, the first five rows of our dataset will be displayed so that we can take a look at the different pieces of data available to us and what kind of information they bring to the analysis.

```{r Visualizing first 5 rows}
head(ireland_houses, 5)
```

Looking at the output of the previous step, some interesting things can be observed:

* __id__ and __daftShortCode__ seem to describe values that look like identifiers, but identifiers that are used for different purposes.

* __title__ contains values which look like addresses of properties.

* __price__ are expected to be of _numeric_ type, nevertheless, strings like _"Price on Application"_ are also present.

* __size_meters_squared__ and __propertySize__, at first glance, contain the same information / values, however, formatted differently. Another interesting point is that there are values missing on both variables.

* __bedrooms__ and __bathrooms__ contain the amount of each of those in the property and should therefore be of _numeric_ type.

* __propertyType__ contain values that describe what kind of property that is, for example: Bungalow and Detached.

* __publishDate__ is an attribute related more to the ad than to the property itself and it describes the date when the ad was published.

* __ber_rating__ is a variable that tells how energy efficient is a property, BER stands for Building Energy Rating.

* __ber_code__ is simply the ID of the certificate that the house was given.

* __ber_epi__ describes the energy consumption per square meter of a the property yearly.

* __latitude__ and __longitude__ are spatial values used to locate the property on the map.

* __category__ only shows the value of _"Buy"_.

* __location__ contains value with the concatenation of town/city and the county.

## Percentage of Missing Values in the dataset per feature

In this step, it is possible to see that there are variables with missing values. Nevertheless, it is worth noting that most of those features have been assigned the type of character and there might be features with missing data, but due to the wrong data type, they are displayed as if they were not missing.

At a later point, those features with the wrong data type will be converted into the appropriate data type.


```{r Percentage of Missing Values}


compile_missing_values_table <- function( dfHouses ) {
  percentage_missing <- colSums(is.na(dfHouses)) * 100 / nrow(dfHouses)
  percentage_missing_sorted <- percentage_missing[order(percentage_missing, decreasing = TRUE)]

  percentage_missing_formatted = percent(formattable(percentage_missing_sorted) / 100)

  kbl(percentage_missing_formatted, col.names = NULL) %>%
    kable_paper(bootstrap_options = "striped", full_width = F, html_font = "Computer Modern") %>%
    add_header_above(header = c("Feature", "Percentage of Missing Values"))
}


compile_missing_values_table( ireland_houses )

```

## Count of unique values per feature in the dataset

In this section, it is possible to understand how many distinct values each feature has. By analysing the results, we can identify that there is likely duplicate observations in the dataset.

For example, the dataset has over 100 thousand observations, but only about 13 thousand different values for : *id*, *daftShortCode* and *url_link*, which are usually used to uniquely identify a record.


```{r Count of Unique Values}

compile_unique_values_table <- function( dfHouses ) {
  
  unique_values_list <- lapply(dfHouses, function(x) length(unique(x)) ) %>% 
    as.list()

  unique_values_sorted <- unique_values_list[ order( unlist(unique_values_list), decreasing = TRUE  ) ]

  unique_values_df <- data.frame( unique_values_sorted )

  kbl(t(unique_values_df)) %>%
    kable_paper(bootstrap_options = "striped", full_width = F, html_font = "Computer Modern") %>%
    kable_styling(fixed_thead = T) %>% 
    add_header_above(header = c("Total Number of Observations", nrow(ireland_houses) ), align = "right") %>%
    add_header_above(header = c("Feature", "Total of Unique Values"))
}

compile_unique_values_table(ireland_houses)

```

## Further Investigation

After doing some data exploration and gaining some insight into the dataset, the next step is to look at the descriptive statistics of the features in the dataset in order to get a general summary of the dataset as a whole and explore properties like:

* **Data types**
* **Min and Max values**
* **Completion rate**
* **Mean**
* **Percentiles**

```{r Further Investigation}
skim(ireland_houses)
```

The output of the skim function shows us that our findings from the initial data exploration are correct. For example, there are over 100 thousand observations in the dataset, but only about 12 thousand unique values, it can also be seen that the datatypes used for some of the features are clearly not right, for instance price is of type character. 

Furthermore, there are features with a high number of missing values, for example: *size* and *ber_code*. There is potentially more missing values on other features, however, those are hidden for now due to the wrong data type being used.

# Data Cleaning

This step will be completed in the following stages:

* *Rename Variables*
* *Removal of Duplicate observations*
* *Convert variables into appropriate data types*
* *Remove observations with missing values*

## Data Transformation - Renaming Variables

It was seen that some of the variables did not follow a naming standard and used an adequate name, for example: *title* actually refers to the *address* of the house and *size_meters_squared* refers to the *size*.

The naming convention used across the variables is __camel-case__, therefore *ber_rating* and *ber_epi* will be renamed to *berRating* and *berEPI* respectively.

```{r Data Transformation - Renaming Variables}
ireland_houses <- rename( ireland_houses, 
  address = title,
  size = size_meters_squared,
  berRating = ber_rating,
  berEPI = ber_epi,
  urlLink = url_link,
  berCode = ber_code
)

```


## Data Cleaning - Removal of Duplicates

As it was seen before, there are many duplicate values, in fact, only about 12% of the observations are unique. In order to remove the duplicate values, the feature *url_link* will be used to filter out any duplicate observations.

The rationale behind using the *url_link* is that URLs are guaranteed to be unique across the internet and they are a concatenation of: property type + address + id. Thus, by removing observations which have the same URL, we are sure to remove observations that are the very same because they were put on sale on the websites more than once and got different ids.

So, in the step below, a new column, *urlNoId*, will be created from the *urlLink* feature, however without the id information.


```{r Data Cleaning - Removal of Duplicates}

remove_id_from_url <- function(url) {
  url_tokens <- str_split(url, pattern = "/")[[1]]
  
  url_tokens_without_id <- stri_paste( url_tokens[1 : length(url_tokens) - 1], collapse = "/"  )
  return(url_tokens_without_id)
}


ireland_houses$urlNoId <- ireland_houses$urlLink %>%
  lapply(remove_id_from_url) %>%
  unlist
  
ireland_houses <- ireland_houses %>%
  distinct(urlNoId, .keep_all = T)


skim(ireland_houses)

compile_missing_values_table(ireland_houses)

```

## Data Transformation - Convert variables into appropriate Data Types

The output of the __skim__ function has showed some interesting details, for instance: *price*, *berEPI*, and *bedrooms* are of type __character__, when, in fact, __numeric__ would be a better type because they represent quantitative data.

*propertyType*, *category* and *berRating* are of type characters, however, __factor__ may be a better type because there is a limited number of categorical values they can hold, __factor__ is also a better type as it allows for better data manipulation so that typos can be avoided and sorting the data in a meaningful way becomes possible.

Thus, in this next step, the data will be tidied up in a better manner.

```{r Data Transformation - Tidying Data}
ireland_houses <- mutate(
  ireland_houses,
  price = parse_number(price),
  berEPI = as.numeric(str_remove(berEPI, "kWh/m2/yr")),
  bedrooms = as.numeric(bedrooms),
  
  propertyType = as.factor(propertyType),
  category = as.factor(category),
  berRating = as.factor(berRating)
)

skim(ireland_houses)
```


## Data Cleaning - Removing observations without Price
Once the date types have been sorted, it is possible to see the presence of missing data in many of the variables in our dataset.

For example: 
* *berEPI* is missing in about *50%* of the observations; 
* *berCode* is missing in about *42%* of the observations; 
* *size* is missing in *26%* of the observations; 
* *price* is missing in about *5%* of the observations;
* *bathrooms* is missing in about *2%* of the observations;
* There are 46 out of 12.586 observations where number of *bedrooms* is missing.

At this stage, the only observations that can be removed are those without the price variable because that is the target variable of the analysis.

```{r Data Cleaning - Removing price}

ireland_houses <- ireland_houses %>% filter(!is.na(price))
skim(ireland_houses)



calc_area_size <- function( price_in_meter, price_in_ac ) {
  if ( is.na(price_in_meter) ) {
    acre_in_meters_ireland = 6555
    my_size <- as.numeric(str_remove(price_in_ac, "ac"))
    return( my_size * acre_in_meters_ireland )
  }
  
  return (price_in_meter)
}

ireland_houses$size <- mapply(calc_area_size, ireland_houses$size, ireland_houses$propertySize)

skim(ireland_houses)

```


```{r Data Exploration - Correlation}
numeric_data <- ireland_houses %>% 
  select(price, size, bedrooms, bathrooms, berCode, berEPI, latitude, longitude) 


skim(numeric_data)

plot( price ~ size, data = numeric_data, main = "Scatter plot of data" )
cor.test( numeric_data$price, numeric_data$size, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$bedrooms, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$bathrooms, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$berCode, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$berEPI, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$latitude, use = "complete.obs"  )
cor( numeric_data$price, numeric_data$longitude, use = "complete.obs"  )


sum( with( numeric_data, bedrooms > 10 | bathrooms > 10  )  )


ireland_houses_lm <- numeric_data %>%
  filter(!is.na(size)) %>%
  
  filter(!is.na(bedrooms)) 
  

#Estimando o modelo
modelo_tempodist <- lm(formula = price ~ bedrooms + size + bathrooms + longitude,
                       data = ireland_houses_lm)

#Observando os parâmetros do modelo_tempodist
summary(modelo_tempodist)


numeric_data %>%
  correlation(method = "pearson") %>%
  plot()

chart.Correlation((numeric_data), histogram = TRUE)

```

```{r Data Cleaning}
ireland_houses_cleaned <- ireland_houses %>%
  filter(!is.na(price)) %>%
  filter(!is.na(propertySize)) %>%
  filter(!is.na(bathrooms)) %>%
  filter(!is.na(bedrooms))
  
skim(ireland_houses_cleaned)
```

The last step removed 22.260 observations, which represents about 22.2% of all the observations.
As the data tidy up process is done, the resulting data frame can be written to disk in CSV format.

## Data Transformation - Deriving other variables

In this step, the address variable will be studied further. The address information is a concatenation of building or apartment number, street name, neighborhood and post town or city. In order to gather more meaningful information from the address variable, the post town or city information will be extracted into another variable called town.

```{r Data Transformation - Deriving other variables}
get_town_from_address <- function(address) {
  address_tokens <- str_split(address, pattern = ",")[[1]]
  town <- str_trim( address_tokens[length(address_tokens)]  )
  return(town)
}

ireland_houses_cleaned$town <- ireland_houses_cleaned$address %>% 
  lapply(get_town_from_address) %>%
  unlist
```

## Write Clean Dataset to Disk

Finally, after doing some initial data exploration and cleaning, we can proceed to start analyzing the variance and covariance of the variables in our dataset, thus diving a little deeper into our analysis.

```{r Write Clean Dataset to Disk}
dataset_filename <- paste(dataset_directory, "ireland_houses_cleaned.csv", sep="")
write.csv(ireland_houses_cleaned, dataset_filename, row.names = FALSE)
```

